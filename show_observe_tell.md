# Summary
Almost image captioning systems now are based on the idea of CNN + LSTM in which CNN is responsible for extracting a static representation, then LSTM leverages this representation to interpret it into sentences. Attention-based and attribute-based approaches are also adopted; however, co-occurrence dependencies are being overlooked. In this paper, they propose a new framework exploiting dependencies among attributes to infer relation terms (object terms: woman, umbrella; relational terms: under, above; descriptive terms: there is a â€¦). Instead of attending on the feature map from CNN which are supposed to be redundant and irrespective, their model uses a RNN with the visual attention to observe before predicting image descriptions. Experiments are conducted on MS-COCO dataset 2014, then compare scores with SotA works.
# Contributions
The main contributions of this paper is as follow:
1) Introducing an attribute detection mechanism using visual attention with inferring attribute dependencies allowing more accurate attention for image captioning. More precisely, they invent a stack attention model in which the first layer of the attention model is to extract fine-grained features; and the second layer then operates the standard attention mechanism to focus on the most reasonable attribute (attribute -driven). Basically, from CNN, they first have a feature map of k regions. At each time step, attention module will attend to the most related region via soft-attention (first attention). Now, they have acquired the context feature, and they also extract a series context features. After that, they continue attending the series of context features (second attention) when generating the next word to get the attribute-based features.
2) Conducting extensive experiments to demonstrate the effectiveness of the proposed framework. 
3) Detailed analysis (ablation and qualitative analysis) are provided for further insights and research in this domain.
# Possible improvement
Although this paper surpasses existing methods, some attempts could be made to gain better results: 1) Besides beam search, teacher forcing could be applied to speed up training and may get better performance. 2) The metrics in this paper are weak although they are written in 2018, BLEU, ROUGE and CIDEr are mostly based-on overlapping calculations between prediction and ground-truth. More advanced metrics like SPICE should be also validated to assure the feasibility of this approach.
